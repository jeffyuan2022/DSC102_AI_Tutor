Dataflow graph / task graph is built under the hood Possible Issue in Dask:
Dask: Task-Parallelism best Practices:
Data Partition sizes:
Avoid too few chunks (low degree of par.)
Avoid too many chunks (task graph overhead) Be mindful of available DRAM Rough guidelines they give:
# data chunks ~ 3x-10x # cores, but # cores x chunk size must be < machine DRAM, but chunk size shouldn’t be too small (~1 GB is OK) Q: Do you tune any of these when using an RDBMS?
Dask still lacks “physical data independence”!
Use the Diagnostics dashboard:
Monitor # tasks, core/node usage, task completion Task Graph sizes:
Too large:
Bottlenecks (serialization / communication / scheduling) Too small: Under-utilization of cores/nodes Rough guidelines:
Tune data chunk size to adjust # tasks (see previous point)
Break up a task/computation
Fuse tasks/computations aka “batching”, or in other cases break jobs apart into distinct stages. Execution Optimization Tradeoffs
Be judicious in tuning data chunk sizes
Be judicious in batching vs breaking up tasks
Speedup is a function of the above factors
Single-Instruction Multiple-Data (SIMD)
A fundamental form of parallel processing in which different chunks of data are processed by the “same” set of instructions shared by multiple processing units (PUs) Aka “vectorized” instruction processing (vs “scalar”)
Data science workloads are very amenable to SIMD
Note: no “master” scheduler in this scenario Single-Instruction Multiple Thread (SIMT): Generalizes notion of SIMD to different threads concurrently doing so Each thread may be assigned a core or a whole PU Single-Program Multiple Data (SPMD): A higher level of abstraction generalizing SIMD operations or programs Under the hood, may use multiple processes or threads
Each chunk of data processed by one core/PU
Applicable to any CPU, not just vectorized PUs
Most common form of parallel programming In this case, work is distributed from a central scheduler or orchestrator.
In data science computations, an often useful surrogate for completion time is the instruction throughput FLOP/s,
i.e., number of floating point operations per second Modern data processing programs, especially deep learning (DL) may have billions of FLOPs aka GFLOPs!
Amdahl’s Law: Formula to upper bound possible speedup A program has 2 parts: one that benefits from multi-core parallelism and one that does not Non-parallel part could be for control, memory stalls, traversing a linked list
Moore's Law: The number of transistors in a dense integrated circuit doubles Practices:
Linear?
Task Graph
DRAM is the level of memory that has the lowest latency to read data from
Dennard Scaling: As transistors get smaller, their power density stays constant, so that the power use stays in proportion with area. Takeaway from hardware trends: it is hard for general-purpose CPUs to sustain FLOP-heavy programs like deep nets
Motivated the rise of “accelerators” for some classes of programs Graphics Processing Unit (GPU): Tailored for matrix/tensor ops Basic idea: use tons of ALUs; massive data parallelism (SIMD on steroids); Titan X offers ~11 TFLOP/s!
Tensor Processing Unit (TPU): Even more specialized tensor ops in DL inference; ~45 TFLOP/s!
Field-Programmable Gate Array (FPGA): Configurable for any class of programs; ~0.5-3 TFLOP/s
(1):Data processing programs need to go through the OS System Call API to read text files but can typically bypass that API if they want to read binary file: FALSE
(2):Which of the following properties of data processing programs is sometimes exploited to help reduce runtimes?: Spatial locality of reference;Temporal locality of reference ;Parallelism in computations
(Post Midterm:)
How much DRAM might a machine have? Common DRAM configs:
• Average Laptop: 16GB
• t2.xlarge EC2 instance: 16GB (at $0.19/hour)
• 2023 MacBook Pro: 32GB-96GB
• Consumer Deep Learning / Gaming PC: 128GB ($288 fixed)
• r7g.metal EC2 instance: 512GB (at $3.43/hour)
• hpc6id.32xlarge EC2 instance: 1024GB (at $5.70/hour)
Less common: u-24tb1.112xlarge: 24TB (at $218.40/hour)
Scalable Data Access
Central Issue: Large data file does not fit entirely in DRAM Basic Idea: Divide-and-conquer again. “Split” a data file (virtually or physically)
and stage reads of its pages from disk to DRAM; vice versa for writes.
Single-node disk: Paged access from file on local disk
Remote read: Paged access from disk(s) over a network
Distributed memory: Data fits on a cluster’s total DRAM
Distributed disk: Use entire memory hierarchy of cluster
Paged Data Access to DRAM
Page Management in DRAM Cache
Caching: Retaining pages read from disk in DRAM
Eviction: Removing a page frame’s content in DRAM
Spilling: Writing out pages from DRAM to disk
❖ If a page in DRAM is “dirty” (i.e., some bytes were written but not backed up on disk), eviction requires a spill. ❖ The set of DRAM-resident pages typically changes over the lifetime of a process
Cache Replacement Policy: The algorithm that chooses which
page frame(s) to evict when a new page has to be cached but the
OS cache in DRAM is full
❖ Popular policies include Least Recently Used, Most Recently
Used, etc. (more shortly)
Quantifying I/O: Disk, Network
Page reads/writes to/from DRAM from/to disk incur latency Disk I/O Cost: Abstract counting of number of page I/Os; can map to bytes given page size
Sometimes, programs read/write data over network Communication/Network I/O Cost: Abstract counting of number of pages/bytes sent/received over network
I/O cost is abstract; mapping to latency is hardware-specific
Example: Suppose a data file is 40GB; page size is 4KB
I/O cost to read file = 10 million page I/Os
Disk with I/O throughput: 800 MB/s → 40GB/800MBps = 50s
Network with speed: 200 MB/s → 40GB/200MBps = 200s
Scaling to (Local) Disk
In general, scalable programs stage access to pages of file on disk and efficiently use available DRAM ❖ Recall that typically DRAM size << Disk size
Modern DRAM sizes can be 10s of GBs; so we read a
“chunk”/“block” of file at a time (say, 1000s of pages) ❖ On magnetic hard disks, such chunking leads to more sequential I/Os, raising throughput and lowering latency!
❖ Similarly, write a chunk of dirtied pages at a time
Generic Cache Replacement Policies
What to do if number of page frames is too few for file? Cache Replacement Policy: Algorithm to decide which page frame(s) to evict to make space
Typical frame ranking criteria:
❖ recency of use
❖ frequency of use
❖ number of processes reading it
Typical optimization goal: Reduce total page I/O costs A few well-known policies:
❖ Least Recently Used (LRU): Evict page that was used longest ago
❖ Most Recently Used (MRU): (Opposite of LRU) ❖ ML-based caching policies are “hot” nowadays!
Data Layouts and Access Patterns
❖ Recall that data layouts and data access patterns affect what data subset gets cached in higher level of memory hierarchy ❖ Recall matrix multiplication example and CPU caches ❖ Key Principle: Optimizing layout of data file on disk based on data access pattern can help reduce I/O costs
❖ Applies to both magnetic hard disk and flash SSDs ❖ But especially critical for magnetic hard disks due to vast differences in latency of random vs sequential access!
Row-store vs Column-store Layouts
❖ A common dichotomy when serializing 2-D structured data
(relations, matrices, DataFrames) to file on disk
❖ Based on data access pattern of program, I/O costs with row- vs col-store can be orders of magnitude apart! ❖ With row-store: need to fetch all pages; I/O cost: 6 pages
❖ With col-store: need to fetch only B’s pages; I/O cost: 2 pages
This difference generalizes to higher dimensions for tensors
Hybrid/Tiled/“Blocked” Layouts
Dask’s DataFrame
Basic Idea: Split data file (virtually or physically) and stage reads of its pages from disk to DRAM (vice versa for writes) ❖ Dask DF scales to disk-resident data via a row-store
❖ “Virtual” split: each split is a Pandas DF under the hood ❖ Dask API is a “wrapper” around Pandas API to scale ops to splits and put all results together
❖ If file is too large for DRAM, need manual repartition() to get physically smaller splits (< ~1GB)
Modin’s DataFrame
Basic Idea: Split data file (virtually or physically) and stage reads of its pages from disk to DRAM (vice versa for writes)
❖ Modin’s DF aims to scale to diskresident data via a tiled store
❖ Enables seamless scaling along both dimensions
❖ Easier use of multi-core parallelism
→ Many in-memory RDBMSs had this, e.g., SAP HANA,
Oracle TimesTen
→ ScaLAPACK had this for matrices
Scaling with Remote Reads
Basic Idea: Split data file (virtually or physically) and stage reads of its pages from disk to DRAM (vice versa for writes) ❖ Similar to scaling to local disk but not “local”:
❖ Stage page reads from remote disk/disks over the network
(e.g., from S3)
❖ More restrictive than scaling with local disk, since spilling is not possible or requires costly network I/Os ❖ OK for a one-shot filescan access pattern
❖ Use DRAM to cache; repl. policies
❖ Can also use smaller local disk as cache
Scaling to Disk: Non-dedup. Project Scaling to Disk: Simple Aggregates:
Similar behavior with Non-dedup
Scaling to Disk: Group By Aggregate Q: But what if hash table > DRAM size?
Program might crash depending on backend implementation. OS may keep swapping pages of hash table to/from disk; aka “thrashing” Q: How to scale to large number of groups?
❖ Divide and conquer! Split up R based on values of A
❖ HT for each split may fit in DRAM alone
❖ Reduce running info. size if possible
Scaling to Disk: Relational Select
❖ Straightforward filescan data access pattern
❖ Read pages/chunks from disk to DRAM one by one
❖ CPU applies predicate to tuples in pages in DRAM
❖ Copy satisfying tuples to temporary output pages
❖ Use LRU for cache replacement, if needed
❖ I/O cost: 6 (read) + output # pages (write)
Scaling to Disk: Relational Select
Scaling to Disk: Matrix Sum of Squares Scalable Matrix/Tensor Algebra:
❖ In general, tiled partitioning is more common for matrix/tensor ops ❖ DRAM-to-disk scaling:
❖ pBDR, SystemDS, and Dask Arrays for matrices
❖ SciDB, Xarray for n-d arrays
❖ CUDA for DRAM-GPU caches scaling of matrix/tensor ops